{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehahs03/medical-image-analysis/blob/main/resnet4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INQ4Yk2_Y-rl",
        "outputId": "04760eef-8902-4aa3-d9e4-86740e890280"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlPTmKabKuU",
        "outputId": "2fb23071-e39b-4f50-b836-b7800866d1dd"
      },
      "source": [
        "%cd /content/drive/MyDrive/medical-image-analysis"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/medical-image-analysis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb19RMs1-_cu"
      },
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import scipy\n",
        "from tensorflow.keras import backend as K\n",
        "import gc\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import json\n",
        "import itertools\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D \n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.preprocessing import image\n",
        "from glob import glob"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2YXrT4J_Mer"
      },
      "source": [
        "def Dataset_loader(DIR, RESIZE, sigmaX=10):\n",
        "    IMG = []\n",
        "    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n",
        "    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n",
        "        PATH = os.path.join(DIR,IMAGE_NAME)\n",
        "        _, ftype = os.path.splitext(PATH)\n",
        "        if ftype == \".jpg\":\n",
        "            img = read(PATH)\n",
        "           \n",
        "            img = cv2.resize(img, (RESIZE,RESIZE))\n",
        "           \n",
        "            IMG.append(np.array(img))\n",
        "    return IMG"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAoAK-7l_ULL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cf36da-4f52-4d98-e8c4-96c203e9d2d2"
      },
      "source": [
        "eczema_train = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/train/Eczema Photos\", 224))\n",
        "melanoma_train = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/train/Melanoma Skin Cancer Nevi and Moles\",224))\n",
        "psoriasis_train = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/train/Psoriasis pictures Lichen Planus and related diseases\",224))\n",
        "eczema_test = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/test/Eczema Photos\",224))\n",
        "melonoma_test = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/test/Melanoma Skin Cancer Nevi and Moles\",224))\n",
        "psoriasis_test = np.array(Dataset_loader(\"/content/drive/MyDrive/medical-image-analysis/test/Psoriasis pictures Lichen Planus and related diseases\", 224))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1235/1235 [04:06<00:00,  5.02it/s]\n",
            "100%|██████████| 463/463 [01:31<00:00,  5.08it/s]\n",
            "100%|██████████| 1405/1405 [04:50<00:00,  4.84it/s]\n",
            "100%|██████████| 309/309 [01:03<00:00,  4.86it/s]\n",
            "100%|██████████| 318/318 [01:03<00:00,  4.98it/s]\n",
            "100%|██████████| 352/352 [01:18<00:00,  4.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOFq80Gi_XID"
      },
      "source": [
        "#labelling \n",
        "eczema_train_label = np.full(len(eczema_train),0)\n",
        "melonoma_train_label = np.full(len(melanoma_train),1)\n",
        "psoriasis_train_label = np.full(len(psoriasis_train),2)\n",
        "eczema_test_label = np.full(len(eczema_test),0)\n",
        "melonoma_test_label = np.full(len(melonoma_test),1)\n",
        "psoriasis_test_label = np.full(len(psoriasis_test),2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuibS0Ab_lTC"
      },
      "source": [
        "X_train = np.concatenate((eczema_train, melanoma_train, psoriasis_train), axis=0)\n",
        "Y_train = np.concatenate((eczema_train_label, melonoma_train_label, psoriasis_train_label), axis=0)\n",
        "X_test = np.concatenate((eczema_test, melonoma_test, psoriasis_test), axis=0)\n",
        "Y_test = np.concatenate((eczema_test_label, melonoma_test_label, psoriasis_test_label), axis = 0)\n",
        "#print(Y_test.shape)\n",
        "#print(X_test.shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46ALqdo_oS8"
      },
      "source": [
        "s = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(s)\n",
        "X_train = X_train[s]\n",
        "Y_train = Y_train[s]\n",
        "\n",
        "s = np.arange(X_test.shape[0])\n",
        "np.random.shuffle(s)\n",
        "#X_test = X_test[s]\n",
        "#Y_test = Y_test[s]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02lI6ES2_rGK"
      },
      "source": [
        "Y_train = to_categorical(Y_train, num_classes= 3)\n",
        "Y_test = to_categorical(Y_test, num_classes= 3)\n",
        "\n",
        "#train and evaluation split\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    X_train, Y_train, \n",
        "    test_size=0.3, \n",
        "    random_state=5\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LI5XNMrgVHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf5d19d-a9bb-4d16-cc37-a942806a9505"
      },
      "source": [
        "pre_trained_model = tensorflow.keras.applications.ResNet50(input_shape=(224,224, 3), include_top=False, weights=\"imagenet\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2LZWem8gAef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e8f286-0406-4667-ea02-85d251d8795a"
      },
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "    #print(layer.name)\n",
        "    if hasattr(layer, 'moving_mean') and hasattr(layer, 'moving_variance'):\n",
        "        layer.trainable = True\n",
        "        K.eval(K.update(layer.moving_mean, K.zeros_like(layer.moving_mean)))\n",
        "        K.eval(K.update(layer.moving_variance, K.zeros_like(layer.moving_variance)))\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "print(len(pre_trained_model.layers))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arThh6CiALfL",
        "outputId": "4141d91b-9490-4304-eaa4-b66ecdc9a68c"
      },
      "source": [
        "last_layer = pre_trained_model.get_layer('conv5_block3_out')\n",
        "print('last layer output shape:', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape: (None, 7, 7, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23NNU55ehETQ"
      },
      "source": [
        "x = tensorflow.keras.layers.Flatten()(last_output)\n",
        "x = tensorflow.keras.layers.Dense(512, activation='relu')(x)\n",
        "x = tensorflow.keras.layers.Dropout(0.5)(x)\n",
        "x = tensorflow.keras.layers.Dense(512, activation='relu')(x)\n",
        "x = tensorflow.keras.layers.Dropout(0.5)(x)\n",
        "x = tensorflow.keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "#Config and compile model\n",
        "\n",
        "model1 = Model(pre_trained_model.input, x)\n",
        "optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyILdlHjAj_k"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX5zYiIaAmOL"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rotation_range=60, width_shift_range=0.2, height_shift_range=0.2,\n",
        "                                   shear_range=0.2, zoom_range=0.2, fill_mode='nearest')\n",
        "\n",
        "train_datagen.fit(X_train)\n",
        "\n",
        "val_datagen = ImageDataGenerator()\n",
        "val_datagen.fit(X_val)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0HfvRrmEy_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80db3a10-5680-4a55-c45d-a651f75b1daf"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 30\n",
        "history0 = model1.fit_generator(train_datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = val_datagen.flow(X_val, Y_val),\n",
        "                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size), \n",
        "                              validation_steps=(X_val.shape[0] // batch_size))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 64s 845ms/step - loss: 2.1185 - accuracy: 0.4796 - val_loss: 146820656.0000 - val_accuracy: 0.4866\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 25s 751ms/step - loss: 1.1631 - accuracy: 0.5057 - val_loss: 397.7205 - val_accuracy: 0.4196\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.9401 - accuracy: 0.5498 - val_loss: 7.1188 - val_accuracy: 0.3750\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.9042 - accuracy: 0.5674 - val_loss: 1.0653 - val_accuracy: 0.4821\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.8927 - accuracy: 0.5740 - val_loss: 0.9781 - val_accuracy: 0.5022\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.8735 - accuracy: 0.5806 - val_loss: 0.8619 - val_accuracy: 0.6138\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 25s 760ms/step - loss: 0.8425 - accuracy: 0.5901 - val_loss: 0.7444 - val_accuracy: 0.6719\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.8452 - accuracy: 0.5996 - val_loss: 0.7354 - val_accuracy: 0.6853\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.8317 - accuracy: 0.6139 - val_loss: 0.7335 - val_accuracy: 0.6942\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.8012 - accuracy: 0.6181 - val_loss: 0.7431 - val_accuracy: 0.6808\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.8030 - accuracy: 0.6162 - val_loss: 0.7162 - val_accuracy: 0.7031\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.7486 - accuracy: 0.6485 - val_loss: 0.7292 - val_accuracy: 0.6808\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.7776 - accuracy: 0.6494 - val_loss: 0.6883 - val_accuracy: 0.7031\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.7594 - accuracy: 0.6380 - val_loss: 0.6693 - val_accuracy: 0.7076\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.7647 - accuracy: 0.6380 - val_loss: 0.6831 - val_accuracy: 0.7165\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.7212 - accuracy: 0.6713 - val_loss: 0.7035 - val_accuracy: 0.6920\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.7198 - accuracy: 0.6689 - val_loss: 0.6873 - val_accuracy: 0.7009\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 25s 755ms/step - loss: 0.7038 - accuracy: 0.6765 - val_loss: 0.7038 - val_accuracy: 0.6964\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.7254 - accuracy: 0.6618 - val_loss: 0.6902 - val_accuracy: 0.6964\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.7189 - accuracy: 0.6826 - val_loss: 0.6545 - val_accuracy: 0.7031\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.6865 - accuracy: 0.6864 - val_loss: 0.6260 - val_accuracy: 0.7411\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.6920 - accuracy: 0.6883 - val_loss: 0.6132 - val_accuracy: 0.7366\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.6748 - accuracy: 0.7059 - val_loss: 0.6875 - val_accuracy: 0.7009\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.6296 - accuracy: 0.7059 - val_loss: 0.7439 - val_accuracy: 0.6920\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.6558 - accuracy: 0.6964 - val_loss: 0.6573 - val_accuracy: 0.7121\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 25s 755ms/step - loss: 0.6751 - accuracy: 0.7011 - val_loss: 0.6534 - val_accuracy: 0.7299\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.6573 - accuracy: 0.7016 - val_loss: 0.6008 - val_accuracy: 0.7388\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.6309 - accuracy: 0.7130 - val_loss: 0.6905 - val_accuracy: 0.7054\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.6401 - accuracy: 0.7068 - val_loss: 0.5809 - val_accuracy: 0.7433\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.6050 - accuracy: 0.7187 - val_loss: 0.6819 - val_accuracy: 0.7232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY3wtqDJd3Wf"
      },
      "source": [
        "np.save('his0_resnet.npy',history0.history)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFo-3ZAikWt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b5e60d-3a47-4e9a-8979-3314608385c7"
      },
      "source": [
        "model1.save(\"0thModel.h5\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwKQytXZpVoQ"
      },
      "source": [
        "**FINE TUNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FXRdFeNnVfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e14296b-d64c-4c5b-ce45-73e57f1f48e3"
      },
      "source": [
        "model4 = Model(pre_trained_model.input, x)\n",
        "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.2, amsgrad=False)\n",
        "model4.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTmmbBoxnGNg"
      },
      "source": [
        "for layer in pre_trained_model.layers[::-5]:\n",
        "    layer.trainable = True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBrdwZ_7nZ0Y"
      },
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.5, \n",
        "                                            min_lr=0.000001, cooldown=2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1v1RoX6nhuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc917ee5-76ae-487b-d7ad-7efbbe0caa76"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 30\n",
        "history3 = model4.fit_generator(train_datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = val_datagen.flow(X_val, Y_val),\n",
        "                              verbose = 1, steps_per_epoch=(X_train.shape[0] // batch_size),\n",
        "                              validation_steps=(X_val.shape[0] // batch_size),\n",
        "                              callbacks=[learning_rate_reduction])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 32s 839ms/step - loss: 0.6377 - acc: 0.7310 - val_loss: 0.7079 - val_acc: 0.7388\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 25s 760ms/step - loss: 0.6074 - acc: 0.7230 - val_loss: 0.6999 - val_acc: 0.7232\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.5710 - acc: 0.7524 - val_loss: 0.6359 - val_acc: 0.7388\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.5618 - acc: 0.7642 - val_loss: 0.6853 - val_acc: 0.7165\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 25s 755ms/step - loss: 0.5916 - acc: 0.7434 - val_loss: 0.7252 - val_acc: 0.7210\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 25s 755ms/step - loss: 0.5798 - acc: 0.7400 - val_loss: 0.6382 - val_acc: 0.7299\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.5945 - acc: 0.7301 - val_loss: 0.6781 - val_acc: 0.7143\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.6141 - acc: 0.7329 - val_loss: 0.6366 - val_acc: 0.7210\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.5736 - acc: 0.7467 - val_loss: 0.6580 - val_acc: 0.7277\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.5796 - acc: 0.7462 - val_loss: 0.6954 - val_acc: 0.7344\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.5809 - acc: 0.7467 - val_loss: 0.7206 - val_acc: 0.7321\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.5977 - acc: 0.7391 - val_loss: 0.6656 - val_acc: 0.7411\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.5909 - acc: 0.7410 - val_loss: 0.6529 - val_acc: 0.7612\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.5468 - acc: 0.7623 - val_loss: 0.6763 - val_acc: 0.7299\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.5606 - acc: 0.7566 - val_loss: 0.6997 - val_acc: 0.7210\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.6032 - acc: 0.7211 - val_loss: 0.6765 - val_acc: 0.7433\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 25s 760ms/step - loss: 0.5563 - acc: 0.7462 - val_loss: 0.6434 - val_acc: 0.7589\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.5714 - acc: 0.7391 - val_loss: 0.6329 - val_acc: 0.7232\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.5881 - acc: 0.7438 - val_loss: 0.6071 - val_acc: 0.7455\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.5919 - acc: 0.7538 - val_loss: 0.6849 - val_acc: 0.7165\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 25s 760ms/step - loss: 0.5770 - acc: 0.7457 - val_loss: 0.6422 - val_acc: 0.7143\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.5945 - acc: 0.7287 - val_loss: 0.6124 - val_acc: 0.7433\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.5521 - acc: 0.7500 - val_loss: 0.5661 - val_acc: 0.7679\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.5871 - acc: 0.7306 - val_loss: 0.6225 - val_acc: 0.7188\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.5907 - acc: 0.7391 - val_loss: 0.7023 - val_acc: 0.6987\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 25s 755ms/step - loss: 0.5647 - acc: 0.7410 - val_loss: 0.6051 - val_acc: 0.7478\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.5869 - acc: 0.7571 - val_loss: 0.6658 - val_acc: 0.7433\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 25s 761ms/step - loss: 0.5958 - acc: 0.7315 - val_loss: 0.6658 - val_acc: 0.7232\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 25s 753ms/step - loss: 0.5921 - acc: 0.7415 - val_loss: 0.6157 - val_acc: 0.7299\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 25s 759ms/step - loss: 0.5707 - acc: 0.7476 - val_loss: 0.6717 - val_acc: 0.7254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECNQAcebs0g6"
      },
      "source": [
        "np.save('his3_resnet.npy',history3.history)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfkewlEdlPGq",
        "outputId": "101a08e4-6e6d-41e1-c01e-add5d569bfe8"
      },
      "source": [
        "model4.save(\"3rdModel.h5\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiKl4-9NJxr5"
      },
      "source": [
        "#from tensorflow.keras.models import load_model\n",
        "#model4 = load_model('0thModel.h5')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuUtbb0Xnpi9",
        "outputId": "2a040afb-4f2e-4bbf-a8a2-16cd1312fed9"
      },
      "source": [
        "loss_test, acc_test = model4.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"Test: accuracy = %f  ;  loss = %f\" % (acc_test, loss_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 4s 108ms/step - loss: 0.5900 - acc: 0.7382\n",
            "Test: accuracy = 0.738241  ;  loss = 0.589956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "2e94a28c",
        "outputId": "fe33a358-6a94-4cef-80e0-a93e793fd050"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix 0f resnet4',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "y_pred = model4.predict(X_test)\n",
        "# Convert predictions classes to one hot vectors \n",
        "y_pred_classes = np.argmax(y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "y_true = np.argmax(Y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(3))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}